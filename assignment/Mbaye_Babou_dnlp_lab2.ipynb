{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ammi_dnlp_lab2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzBHf68UXD2k",
        "colab_type": "text"
      },
      "source": [
        "# Chit Chat Chatbots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm6rTDCrXJ4V",
        "colab_type": "text"
      },
      "source": [
        "In the previous lab, we explored models that try to answer questions by reasoning over free-text input. In this lab, we will explore two types of models to create chatbots.\n",
        "\n",
        "First, let's consider important qualities for a chit-chat chatbot system\n",
        "\n",
        "\n",
        "1.   **Readability** - whatever model we use, the chats it creates should be easily understood by humans\n",
        "2.   **Consistency** - when chatting with a chatbot, the bot should maintain consistent information. Imagine a bot that says \"Hi I'm Jack'' and then \"Hello, my name is Jane\" - quite confusing\n",
        "3.    **Engaging** - To encourage users to talk to the bot, the bot should be able to generate interesting, engaging responses. If the only response was \"wow, that's cool,\" users are quite unlikely to want to talk very much to the chat bot\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrYcmfoI5PWO",
        "colab_type": "code",
        "outputId": "467f04d5-1dbe-4ff5-e974-ffc8d0f40f6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "%%html\n",
        "<p style='color: blue;'>\n",
        "  Throughout the lab, there will be <b>questions</b> you should answer. <b>All questions you need to write an answer to will be in this blue color.</b>\n",
        "  \n",
        "  <br>Please write brief answers- no need for long explanations. \n",
        "  <br>There can be multiple correct answers to the questions.\n",
        "  \n",
        "  <br><br>The goal of these questions is to:\n",
        "  <ul style='color: green;'>\n",
        "    <li> Review the lecture material in the context of practical models and develop intuition about the models\n",
        "    <li> Develop a sense of experimentation - we will pretend we have a dataset and will walk through an experimental thought process.\n",
        "  </ul>\n",
        "\n",
        "<b>We are going to do the lab as a group. <br>I will explain the sections in more depth, as we did not cover dialogue deeply during the lecture. <br> After we discuss, I will provide time for you to write a few sentences. At the end of the lab, you will hand it in. In theory, everyone should be finished together!</b>\n",
        "\n",
        "  \n",
        "</p>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style='color: blue;'>\n",
              "  Throughout the lab, there will be <b>questions</b> you should answer. <b>All questions you need to write an answer to will be in this blue color.</b>\n",
              "  \n",
              "  <br>Please write brief answers- no need for long explanations. \n",
              "  <br>There can be multiple correct answers to the questions.\n",
              "  \n",
              "  <br><br>The goal of these questions is to:\n",
              "  <ul style='color: green;'>\n",
              "    <li> Review the lecture material in the context of practical models and develop intuition about the models\n",
              "    <li> Develop a sense of experimentation - we will pretend we have a dataset and will walk through an experimental thought process.\n",
              "  </ul>\n",
              "\n",
              "<b>We are going to do the lab as a group. <br>I will explain the sections in more depth, as we did not cover dialogue deeply during the lecture. <br> After we discuss, I will provide time for you to write a few sentences. At the end of the lab, you will hand it in. In theory, everyone should be finished together!</b>\n",
              "\n",
              "  \n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3zg0WczXjDs",
        "colab_type": "text"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kguDIJbLXmZU",
        "colab_type": "text"
      },
      "source": [
        "The dataset we will use for this lab is called `PersonaChat` - it was created to directly address problem 2. Each person talking in the dataset has a personality, which helps maintain consistency in the dialogue. We saw it last week in the tutorials as well (when you worked through beam search)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpvTb8bBX40A",
        "colab_type": "code",
        "outputId": "f139a553-dd77-4318-fce7-2f520004ddf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!git clone https://github.com/facebookresearch/ParlAI.git ~/ParlAI  > /dev/null\n",
        "!cd ~/ParlAI; python setup.py develop > /dev/null"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/root/ParlAI'...\n",
            "remote: Enumerating objects: 244, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/244)\u001b[K\rremote: Counting objects:   1% (3/244)\u001b[K\rremote: Counting objects:   2% (5/244)\u001b[K\rremote: Counting objects:   3% (8/244)\u001b[K\rremote: Counting objects:   4% (10/244)\u001b[K\rremote: Counting objects:   5% (13/244)\u001b[K\rremote: Counting objects:   6% (15/244)\u001b[K\rremote: Counting objects:   7% (18/244)\u001b[K\rremote: Counting objects:   8% (20/244)\u001b[K\rremote: Counting objects:   9% (22/244)\u001b[K\rremote: Counting objects:  10% (25/244)\u001b[K\rremote: Counting objects:  11% (27/244)\u001b[K\rremote: Counting objects:  12% (30/244)\u001b[K\rremote: Counting objects:  13% (32/244)\u001b[K\rremote: Counting objects:  14% (35/244)\u001b[K\rremote: Counting objects:  15% (37/244)\u001b[K\rremote: Counting objects:  16% (40/244)\u001b[K\rremote: Counting objects:  17% (42/244)\u001b[K\rremote: Counting objects:  18% (44/244)\u001b[K\rremote: Counting objects:  19% (47/244)\u001b[K\rremote: Counting objects:  20% (49/244)\u001b[K\rremote: Counting objects:  21% (52/244)\u001b[K\rremote: Counting objects:  22% (54/244)\u001b[K\rremote: Counting objects:  23% (57/244)\u001b[K\rremote: Counting objects:  24% (59/244)\u001b[K\rremote: Counting objects:  25% (61/244)\u001b[K\rremote: Counting objects:  26% (64/244)\u001b[K\rremote: Counting objects:  27% (66/244)\u001b[K\rremote: Counting objects:  28% (69/244)\u001b[K\rremote: Counting objects:  29% (71/244)\u001b[K\rremote: Counting objects:  30% (74/244)\u001b[K\rremote: Counting objects:  31% (76/244)\u001b[K\rremote: Counting objects:  32% (79/244)\u001b[K\rremote: Counting objects:  33% (81/244)\u001b[K\rremote: Counting objects:  34% (83/244)\u001b[K\rremote: Counting objects:  35% (86/244)\u001b[K\rremote: Counting objects:  36% (88/244)\u001b[K\rremote: Counting objects:  37% (91/244)\u001b[K\rremote: Counting objects:  38% (93/244)\u001b[K\rremote: Counting objects:  39% (96/244)\u001b[K\rremote: Counting objects:  40% (98/244)\u001b[K\rremote: Counting objects:  41% (101/244)\rremote: Counting objects:  42% (103/244)\u001b[K\rremote: Counting objects:  43% (105/244)\u001b[K\rremote: Counting objects:  44% (108/244)\u001b[K\rremote: Counting objects:  45% (110/244)\u001b[K\rremote: Counting objects:  46% (113/244)\u001b[K\rremote: Counting objects:  47% (115/244)\u001b[K\rremote: Counting objects:  48% (118/244)\u001b[K\rremote: Counting objects:  49% (120/244)\u001b[K\rremote: Counting objects:  50% (122/244)\u001b[K\rremote: Counting objects:  51% (125/244)\u001b[K\rremote: Counting objects:  52% (127/244)\u001b[K\rremote: Counting objects:  53% (130/244)\u001b[K\rremote: Counting objects:  54% (132/244)\u001b[K\rremote: Counting objects:  55% (135/244)\u001b[K\rremote: Counting objects:  56% (137/244)\u001b[K\rremote: Counting objects:  57% (140/244)\u001b[K\rremote: Counting objects:  58% (142/244)\u001b[K\rremote: Counting objects:  59% (144/244)\u001b[K\rremote: Counting objects:  60% (147/244)\u001b[K\rremote: Counting objects:  61% (149/244)\u001b[K\rremote: Counting objects:  62% (152/244)\u001b[K\rremote: Counting objects:  63% (154/244)\u001b[K\rremote: Counting objects:  64% (157/244)\u001b[K\rremote: Counting objects:  65% (159/244)\u001b[K\rremote: Counting objects:  66% (162/244)\u001b[K\rremote: Counting objects:  67% (164/244)\u001b[K\rremote: Counting objects:  68% (166/244)\u001b[K\rremote: Counting objects:  69% (169/244)\u001b[K\rremote: Counting objects:  70% (171/244)\u001b[K\rremote: Counting objects:  71% (174/244)\u001b[K\rremote: Counting objects:  72% (176/244)\u001b[K\rremote: Counting objects:  73% (179/244)\u001b[K\rremote: Counting objects:  74% (181/244)\u001b[K\rremote: Counting objects:  75% (183/244)\u001b[K\rremote: Counting objects:  76% (186/244)\u001b[K\rremote: Counting objects:  77% (188/244)\u001b[K\rremote: Counting objects:  78% (191/244)\u001b[K\rremote: Counting objects:  79% (193/244)\u001b[K\rremote: Counting objects:  80% (196/244)\u001b[K\rremote: Counting objects:  81% (198/244)\u001b[K\rremote: Counting objects:  82% (201/244)\u001b[K\rremote: Counting objects:  83% (203/244)\u001b[K\rremote: Counting objects:  84% (205/244)\u001b[K\rremote: Counting objects:  85% (208/244)\u001b[K\rremote: Counting objects:  86% (210/244)\u001b[K\rremote: Counting objects:  87% (213/244)\u001b[K\rremote: Counting objects:  88% (215/244)\u001b[K\rremote: Counting objects:  89% (218/244)\u001b[K\rremote: Counting objects:  90% (220/244)\u001b[K\rremote: Counting objects:  91% (223/244)\u001b[K\rremote: Counting objects:  92% (225/244)\u001b[K\rremote: Counting objects:  93% (227/244)\u001b[K\rremote: Counting objects:  94% (230/244)\u001b[K\rremote: Counting objects:  95% (232/244)\u001b[K\rremote: Counting objects:  96% (235/244)\u001b[K\rremote: Counting objects:  97% (237/244)\u001b[K\rremote: Counting objects:  98% (240/244)\u001b[K\rremote: Counting objects:  99% (242/244)\u001b[K\rremote: Counting objects: 100% (244/244)\u001b[K\rremote: Counting objects: 100% (244/244), done.\u001b[K\n",
            "remote: Compressing objects: 100% (144/144), done.\u001b[K\n",
            "remote: Total 30015 (delta 149), reused 143 (delta 97), pack-reused 29771\u001b[K\n",
            "Receiving objects: 100% (30015/30015), 57.68 MiB | 26.06 MiB/s, done.\n",
            "Resolving deltas: 100% (21333/21333), done.\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "/usr/local/lib/python3.6/dist-packages/setuptools/dist.py:454: UserWarning: Normalizing '2019.08.19' to '2019.8.19'\n",
            "  warnings.warn(tmpl.format(**locals()))\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kfolded_char_at\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:10625:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kfolded_len\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kfolded_len\u001b[m\u001b[K;\n",
            "         \u001b[01;35m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kfuzzy_match_group_fld\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:11503:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kdata.new_text_pos\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
            "     if (!\u001b[01;35m\u001b[Krecord_fuzzy(state, data.fuzzy_type, data.new_text_pos - data.step)\u001b[m\u001b[K)\n",
            "          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kfuzzy_match_string_fld\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:11270:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kdata.new_text_pos\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
            "     if (!\u001b[01;35m\u001b[Krecord_fuzzy(state, data.fuzzy_type, data.new_text_pos - data.step)\u001b[m\u001b[K)\n",
            "          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kbasic_match\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:11608:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kdata.new_text_pos\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
            "     if (!\u001b[01;35m\u001b[Krecord_fuzzy(state, data.fuzzy_type, data.new_text_pos - data.step)\u001b[m\u001b[K)\n",
            "          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:11522:18:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[Kdata.new_text_pos\u001b[m\u001b[K’ was declared here\n",
            "     RE_FuzzyData \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K;\n",
            "                  \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:11369:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kdata.new_text_pos\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
            "     if (!\u001b[01;35m\u001b[Krecord_fuzzy(state, data.fuzzy_type, data.new_text_pos - data.step)\u001b[m\u001b[K)\n",
            "          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kregex_3/_regex.c:11288:18:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[Kdata.new_text_pos\u001b[m\u001b[K’ was declared here\n",
            "     RE_FuzzyData \u001b[01;36m\u001b[Kdata\u001b[m\u001b[K;\n",
            "                  \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "regex.__pycache__._regex.cpython-36: module references __file__\n",
            "In file included from \u001b[01m\u001b[Kext/_yaml.c:591:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kext/_yaml.h:2:10:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kyaml.h: No such file or directory\n",
            " #include \u001b[01;31m\u001b[K<yaml.h>\u001b[m\u001b[K\n",
            "          \u001b[01;31m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "compilation terminated.\n",
            "Error compiling module, falling back to pure Python\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "warning: no files found matching 'README.txt'\n",
            "warning: no files found matching 'Makefile' under directory '*.txt'\n",
            "warning: no previously-included files matching '*~' found anywhere in distribution\n",
            "warning: no previously-included files found matching '.pre-commit-hooks.yaml'\n",
            "warning: no previously-included files found matching '.travis.yml'\n",
            "warning: no previously-included files found matching 'Makefile'\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "warning: no previously-included files found matching '.travis.yml'\n",
            "warning: no previously-included files found matching 'Makefile'\n",
            "warning: no previously-included files found matching 'test_acid.py'\n",
            "zip_safe flag not set; analyzing archive contents...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OJJ-L5RCyYT",
        "colab_type": "text"
      },
      "source": [
        "**Example: **\n",
        "\n",
        "your persona: i just started college.\n",
        "\n",
        "your persona: i have 3 science classes.\n",
        "\n",
        "your persona: i work part time in the campus library.\n",
        "\n",
        "your persona: i am living at home but hope to live in the dorms next year.\n",
        "\n",
        "**Partner Dialogue**: hi how are you doing\n",
        "\n",
        "**Your Response**: great ! just got off work and relaxing before i study"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWk1ioQMVfiy",
        "colab_type": "code",
        "outputId": "43ddbf43-de88-4cf4-fd7c-a982fbf5217d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 970
        }
      },
      "source": [
        "!python ~/ParlAI/examples/display_data.py --help"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: display_data.py [-h] [-o INIT_OPT] [--show-advanced-args] [-t TASK]\n",
            "                       [-dt {train,train:stream,train:ordered,train:ordered:stream,train:stream:ordered,train:evalmode,train:evalmode:stream,train:evalmode:ordered,train:evalmode:ordered:stream,train:evalmode:stream:ordered,valid,valid:stream,test,test:stream}]\n",
            "                       [-nt NUMTHREADS] [-bs BATCHSIZE]\n",
            "                       [-dynb {None,batchsort,full}] [-dp DATAPATH] [-m MODEL]\n",
            "                       [-mf MODEL_FILE] [-im INIT_MODEL] [-n NUM_EXAMPLES]\n",
            "                       [-mdl MAX_DISPLAY_LEN]\n",
            "                       [--display-ignore-fields DISPLAY_IGNORE_FIELDS] [-v]\n",
            "\n",
            "Display data from a task\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help\n",
            "      show this help message and exit\n",
            "  -n, -ne, --num-examples NUM_EXAMPLES\n",
            "  -mdl, --max-display-len MAX_DISPLAY_LEN\n",
            "  --display-ignore-fields DISPLAY_IGNORE_FIELDS\n",
            "  -v, --display-verbose\n",
            "      If false, simple converational view, does not show other message fields.\n",
            "      (default: False)\n",
            "\n",
            "Main ParlAI Arguments:\n",
            "  -o, --init-opt INIT_OPT\n",
            "      Path to json file of options. Note: Further Command-line arguments\n",
            "      override file-based options. (default: None)\n",
            "  --show-advanced-args\n",
            "      Show hidden command line options (advanced users only) (default: False)\n",
            "  -t, --task TASK\n",
            "      ParlAI task(s), e.g. \"babi:Task1\" or \"babi,cbt\" (default: None)\n",
            "  -dt, --datatype {train,train:stream,train:ordered,train:ordered:stream,train:stream:ordered,train:evalmode,train:evalmode:stream,train:evalmode:ordered,train:evalmode:ordered:stream,train:evalmode:stream:ordered,valid,valid:stream,test,test:stream}\n",
            "      choose from: train, train:ordered, valid, test. to stream data add\n",
            "      \":stream\" to any option (e.g., train:stream). by default: train is random\n",
            "      with replacement, valid is ordered, test is ordered. (default:\n",
            "      train:stream)\n",
            "  -nt, --numthreads NUMTHREADS\n",
            "      number of threads. Used for hogwild if batchsize is 1, else for number of\n",
            "      threads in threadpool loading, (default: 1)\n",
            "  -bs, --batchsize BATCHSIZE\n",
            "      batch size for minibatch training schemes (default: 1)\n",
            "  -dynb, --dynamic-batching {None,batchsort,full}\n",
            "      Use dynamic batching (default: None)\n",
            "  -dp, --datapath DATAPATH\n",
            "      path to datasets, defaults to {parlai_dir}/data (default: None)\n",
            "\n",
            "ParlAI Model Arguments:\n",
            "  -m, --model MODEL\n",
            "      the model class name. can match parlai/agents/<model> for agents in that\n",
            "      directory, or can provide a fully specified module for `from X import Y`\n",
            "      via `-m X:Y` (e.g. `-m parlai.agents.seq2seq.seq2seq:Seq2SeqAgent`)\n",
            "      (default: None)\n",
            "  -mf, --model-file MODEL_FILE\n",
            "      model file name for loading and saving models (default: None)\n",
            "  -im, --init-model INIT_MODEL\n",
            "      load model weights and dict from this file (default: None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe72XiqFXCS0",
        "colab_type": "code",
        "outputId": "c53d6bf5-85ed-4a8f-ff35-a90f36135e20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# let's download and take a look at some examples of data in PersonaChat\n",
        "!python ~/ParlAI/examples/display_data.py --num-examples 10 --task personachat --datatype train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ optional arguments: ] \n",
            "[  display_ignore_fields: agent_reply ]\n",
            "[  display_verbose: False ]\n",
            "[  max_display_len: 1000 ]\n",
            "[  num_examples: 10 ]\n",
            "[ Main ParlAI Arguments: ] \n",
            "[  batchsize: 1 ]\n",
            "[  datapath: /root/ParlAI/data ]\n",
            "[  datatype: train ]\n",
            "[  download_path: /root/ParlAI/downloads ]\n",
            "[  dynamic_batching: None ]\n",
            "[  hide_labels: False ]\n",
            "[  image_mode: raw ]\n",
            "[  init_opt: None ]\n",
            "[  multitask_weights: [1] ]\n",
            "[  numthreads: 1 ]\n",
            "[  show_advanced_args: False ]\n",
            "[  task: personachat ]\n",
            "[ ParlAI Model Arguments: ] \n",
            "[  dict_class: None ]\n",
            "[  init_model: None ]\n",
            "[  model: None ]\n",
            "[  model_file: None ]\n",
            "[ ParlAI Image Preprocessing Arguments: ] \n",
            "[  image_cropsize: 224 ]\n",
            "[  image_size: 256 ]\n",
            "[ Current ParlAI commit: 57f946facd540e1ca6113841d3fc0041d7c00460 ]\n",
            "[creating task(s): personachat]\n",
            "[building data: /root/ParlAI/data/Persona-Chat]\n",
            "[ downloading: http://parl.ai/downloads/personachat/personachat.tgz to /root/ParlAI/data/Persona-Chat/personachat.tgz ]\n",
            "Downloading personachat.tgz: 100% 223M/223M [00:22<00:00, 10.0MB/s]\n",
            "[ Checksum Successful ]\n",
            "unpacking personachat.tgz\n",
            "[loading fbdialog data:/root/ParlAI/data/Persona-Chat/personachat/train_self_original.txt]\n",
            "\u001b[1;37;41m    - - - NEW EPISODE: personachat - - -       \u001b[0;0m\n",
            "\u001b[0;37;40myour persona: i just started college.\n",
            "your persona: i have 3 science classes.\n",
            "your persona: i work part time in the campus library.\n",
            "your persona: i am living at home but hope to live in the dorms next year.\n",
            "hi how are you doing\u001b[0;0m\n",
            "   \u001b[1;94mgreat ! just got off work and relaxing before i study\u001b[0;0m\n",
            "\u001b[0;37;40mnice where do you work\u001b[0;0m\n",
            "   \u001b[1;94mthe library on campus , it is part time but really peaceful and easy\u001b[0;0m\n",
            "\u001b[0;37;40mthat is cool i just live off my girlfriends salary\u001b[0;0m\n",
            "   \u001b[1;94mi live at home , next year i want to move on campus though\u001b[0;0m\n",
            "\u001b[0;37;40mthat is cool do you like to cook\u001b[0;0m\n",
            "   \u001b[1;94mi do , i make this one dish with pork marinated in catalina dressing , its good\u001b[0;0m\n",
            "\u001b[0;37;40mthat sounds so interesting i like to cook brunch on the weekends\u001b[0;0m\n",
            "   \u001b[1;94mi do not cook a lot . this semester i have 3 different science classes\u001b[0;0m\n",
            "\u001b[0;37;40mwow is that your major ?\u001b[0;0m\n",
            "   \u001b[1;94mchemistry , but have a minor in biology\u001b[0;0m\n",
            "\u001b[0;37;40mwow what do you want to do after school\u001b[0;0m\n",
            "   \u001b[1;94mideally i want to work for an energy company working on new energy and fuel sources\u001b[0;0m\n",
            "\u001b[0;37;40mcool i wish i knew more about science\u001b[0;0m\n",
            "   \u001b[1;94mit is difficult but i do enjoy it , just takes a lot of studying\u001b[0;0m\n",
            "\u001b[1;37;41m    - - - NEW EPISODE: personachat - - -       \u001b[0;0m\n",
            "\u001b[0;37;40myour persona: i have borderline personality disorder.\n",
            "your persona: it is my universe , and everyone else is just a character in it.\n",
            "your persona: i work as a dental assistant in a ritzy part of town.\n",
            "your persona: at night , i party hard in the atlanta club scene , and i never miss a music festival.\n",
            "how are you doing today ?\u001b[0;0m\n",
            "   \u001b[1;94mgreat ! just got back from some dbt therapy . it really helps me\u001b[0;0m\n",
            "\u001b[0;37;40mwhat is that ? i am not familiar .\u001b[0;0m\n",
            "   \u001b[1;94mdialectical behavioral therapy . i use it to help with my borderline personality\u001b[0;0m\n",
            "[ loaded 8939 episodes with a total of 65719 examples ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3Ob4tEt1vNP",
        "colab_type": "code",
        "outputId": "a8c28cb0-c71f-4025-dac2-aff2e8c9ac55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "%%html\n",
        "<p style='color: blue;'>\n",
        "  <b>Questions:</b>\n",
        "  <ul style='color: blue;'>\n",
        "    <li>What do the personalities look like?</li>\n",
        "    <li>How does creating bots with these simple personalities address consistency for chatbots? </li>\n",
        "    <li>What are some drawbacks/limitations of these specific personalities for addressing the problem of consistency?</li>\n",
        "  </ul>\n",
        "</p>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style='color: blue;'>\n",
              "  <b>Questions:</b>\n",
              "  <ul style='color: blue;'>\n",
              "    <li>What do the personalities look like?</li>\n",
              "    <li>How does creating bots with these simple personalities address consistency for chatbots? </li>\n",
              "    <li>What are some drawbacks/limitations of these specific personalities for addressing the problem of consistency?</li>\n",
              "  </ul>\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8OS1sBGZN0e",
        "colab_type": "text"
      },
      "source": [
        "- What do the personalities look like?  \n",
        " **anwser :** personnality is simple and gives specific detail.\n",
        "\n",
        "- How does creating bots with these simple personalities address consistency for chatbots?  \n",
        " **anwser :** the bot extract existing fact from personality to stay consistent.\n",
        "\n",
        "- What are some drawbacks/limitations of these specific personalities for addressing the problem of consistency?  \n",
        " **anwser :** limited to the information in the dataset.  \n",
        " (eg. lack of consistency for the questions outside the domain)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kyhri6NvYKda",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "***Let's understand how much data we have. Let's compute the following using ParlAI:***\n",
        "\n",
        "\n",
        "1.   **How many turns of data do we have?** In dialogue datasets, \"amount of data\" is measured in dialogue turns. Each time there is a single line of dialogue, that is called a \"turn\"\n",
        "2.   **On average, how many words form a model input?**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30SF6hN6DAjD",
        "colab_type": "code",
        "outputId": "aec03dd0-8134-4cf1-9747-0d3c3a358295",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        }
      },
      "source": [
        "!python ~/ParlAI/parlai/scripts/data_stats.py -t personachat -dt train -ltim 10000"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ note: changing datatype from train to train:ordered ]\n",
            "[creating task(s): personachat]\n",
            "[loading fbdialog data:/root/ParlAI/data/Persona-Chat/personachat/train_self_original.txt]\n",
            "[ loaded 8939 episodes with a total of 65719 examples ]\n",
            "16s elapsed:\n",
            "      %done  \\\n",
            "    100.00%   \n",
            "      exs  \\\n",
            "    65719   \n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                 stats  \\\n",
            "    \n",
            "input:\n",
            "   utterances: 65719\n",
            "   avg utterance length: 18.356974390967604\n",
            "   tokens: 1206402\n",
            "   unique tokens: 14209\n",
            "   unique utterances: 64580\n",
            "labels:\n",
            "   utterances: 65719\n",
            "   avg utterance length: 11.929411585690591\n",
            "   tokens: 783989\n",
            "   unique tokens: 14507\n",
            "   unique utterances: 64119\n",
            "both:\n",
            "   utterances: 131438\n",
            "   avg utterance length: 15.143192988329098\n",
            "   tokens: 1990391\n",
            "   unique tokens: 18741\n",
            "   unique utterances: 128197\n",
            "   \n",
            "   time_left  \n",
            "          0s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vwk7krxU57M",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "How are dialogue models evaluated?\n",
        "\n",
        "\n",
        "\n",
        "1.   **Automatic Evaluation**: Hits @ 1, Hits @ 5, Hits @ 10, F1\n",
        "2.   **Human Evaluation**: Pairing Selection, Human Rating\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGI2fFPLVRSJ",
        "colab_type": "code",
        "outputId": "712337ec-cf8a-4781-fe81-fe490772da70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "%%html\n",
        "<p style='color: blue;'>\n",
        "  <b>Questions:</b>\n",
        "  <ul style='color: blue;'>\n",
        "    <li>Take some notes about what these metrics are and what they mean here</li>\n",
        "  </ul>\n",
        "</p>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style='color: blue;'>\n",
              "  <b>Questions:</b>\n",
              "  <ul style='color: blue;'>\n",
              "    <li>Take some notes about what these metrics are and what they mean here</li>\n",
              "  </ul>\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugpWX9dnYbdP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Take some notes about what these metrics are and what they mean here?  \n",
        "**answer :**\n",
        "1.   **Automatic Evaluation**: is cheap but less accurate  \n",
        "Hits @ 1:  \n",
        "Hits @ 5: if the good  \n",
        "Hits @ 10:  \n",
        "F1:  \n",
        "2.   **Human Evaluation**: expensive  \n",
        "Pairing Selection :  \n",
        "Human Rating :  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpkwidfAZ_7p",
        "colab_type": "text"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuqnjqxbZ_W2",
        "colab_type": "text"
      },
      "source": [
        "There are two main kinds of dialogue models. \n",
        "\n",
        "*Retrieval* Models analyze the current dialogue context and try to find appropriate responses in the dataset.\n",
        "\n",
        "*Generative* Models analyze the current dialogue context\n",
        "and try to write an answer, word by word, from left to right.\n",
        "This can be thought of as an application of sequence-to-sequence models,  where the \"encoder side\" is the dialogue history and the \"decoder side\" is the dialogue response your chatbot should generate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPRp1MlH3n0h",
        "colab_type": "code",
        "outputId": "ffa4cc35-7327-4545-b26f-5491b43b8140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "%%html\n",
        "<p style='color: blue;'>\n",
        "  <b>Questions:</b>\n",
        "  <ul style='color: blue;'>\n",
        "    <li>Let's discuss the pros/cons of retrieval compared to generative models - Are there settings when you might want to use one over the other?</li>\n",
        "    <li>Compare a chit-chat application to something like booking a movie ticket- would you want to use generative, retrieval, or something else to accomplish that task? Why?</li>\n",
        "    <li>How can you evaluate generative models with the metrics we discussed before? How do you think they will perform compared to retrieval models?</li>\n",
        "    <li>In lecture, Antoine mentioned issues with generative model generation being generic and short. How does this happen in beam search?</li>\n",
        "</ul>\n",
        "\n",
        "<b> If you would like me to discuss how to actually use generative models in dialog, please say something! Otherwise, we will skip.</b>\n",
        "</p>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style='color: blue;'>\n",
              "  <b>Questions:</b>\n",
              "  <ul style='color: blue;'>\n",
              "    <li>Let's discuss the pros/cons of retrieval compared to generative models - Are there settings when you might want to use one over the other?</li>\n",
              "    <li>Compare a chit-chat application to something like booking a movie ticket- would you want to use generative, retrieval, or something else to accomplish that task? Why?</li>\n",
              "    <li>How can you evaluate generative models with the metrics we discussed before? How do you think they will perform compared to retrieval models?</li>\n",
              "    <li>In lecture, Antoine mentioned issues with generative model generation being generic and short. How does this happen in beam search?</li>\n",
              "</ul>\n",
              "\n",
              "<b> If you would like me to discuss how to actually use generative models in dialog, please say something! Otherwise, we will skip.</b>\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwr6OUkPaJr6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "- Let's discuss the pros/cons of retrieval compared to generative models - Are there settings when you might want to use one over the other?  \n",
        " **answer :** \n",
        " - retrieval :\n",
        "   - pros: dont make gramatical mistake, easy to evaluate\n",
        "   - cons: cannot adapt on new situation\n",
        " - generative :\n",
        "   - pros: can generate new text adapted to the context\n",
        "   - cons: need a lot of data, can make mistake, expensive to evaluate\n",
        "\n",
        "- Compare a chit-chat application to something like booking a movie ticket- would you want to use generative, retrieval, or something else to accomplish that task? Why?  \n",
        "\n",
        " **answer :**  \n",
        " - for no goal oriented dialog like chit-chat application :\n",
        "   - generative : is better but risky\n",
        "   - retrivial : is more secure\n",
        " - for goal-oriented dialog like booking a movie ticket :\n",
        "   - retrivial : because we already know the domain\n",
        "   - generative : if we have enough data to train it and risk of bad anwser is not too high\n",
        "\n",
        "- How can you evaluate generative models with the metrics we discussed before? How do you think they will perform compared to retrieval models?  \n",
        "  \n",
        " **answer :** human evaluation to rate: consistency, fluency, engagingness and persona detection  \n",
        "\n",
        " **Compare to the retrivial:**\n",
        " - consistency can be lower for the generative because we can have non sense sequence \n",
        " - fluency can be more fluent\n",
        " - engagingness can be better for the generative because the dialoges are not the same\n",
        " - persona detection: can be the same because persona detection is the ability of model to extract information from the interaction with the person\n",
        "\n",
        "- In lecture, Antoine mentioned issues with generative model generation being generic and short. How does this happen in beam search?  \n",
        " **answer :** beam search maximize the probability over the sentences  which allow the beam search to penalise long sentences and sequences of words that doesn't appears often in the dataset (eg. original sentence).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_99CoAwaW2w",
        "colab_type": "text"
      },
      "source": [
        "### Retrieval Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAfk4Km8bY_Y",
        "colab_type": "text"
      },
      "source": [
        "Let's train a model to do retrieval first. We will try the *Memory Net.* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyNOObOkbYTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can train a model with the following command: \n",
        "# !python ~/ParlAI/examples/train_model.py -m kv_memnn -t personachat -dt train -veps 0.25 --model-file persona_chat_retrieval_model -vmt accuracy\n",
        "\n",
        "# but we have limited time in the tutorial, so let's use an already pretrained model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gylhJvZwa0Vp",
        "colab_type": "text"
      },
      "source": [
        "Quick Parameter Refresher:\n",
        "\n",
        "\n",
        "*  `-m ` means which model we're going to use. Recall retrieval models are trained to rank the true response higher over a set of potential responses from the dataset (in ParlAI, these are called the \"label candidates\"). When it's time to write a dialogue response, the retrieval model returns the response that is ranked the highest\n",
        "*  -`t` refers to the task. Here, we are training on PersonaChat data.\n",
        "* `-dt` refers to the data split. We want to train our model, so we are using the training set.\n",
        "* `-veps` refers to how often we should evaluate during training, our performance on validation. recall this is important because models, particularly neural ones, have the capacity to memorize the training dataset. So it's important to check how the model is doing on the validation set.\n",
        "* `--model-file` refers to when your model is saved, what should the filename be\n",
        "*  `-vmt` refers to the metric which we'll use to decide which model is the best. We'll cover this in the next section\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6wdfxEccAAp",
        "colab_type": "text"
      },
      "source": [
        "**Let's interact with the model to get a sense of what it's learning. **How is this chat going to work?\n",
        "\n",
        "\n",
        "\n",
        "1.   You will be assigned a persona. You will chat to the model by typing in the chat box.\n",
        "2.   The chatbot also has a persona. It's secret and hidden from you!\n",
        "3.   When you've finished chatting with this bot, type [DONE] and a new model persona will be assigned to the bot, so you can talk to a new bot. \n",
        "4.   When you move on to the next chatbot persona, the previous persona will be revealed. \n",
        "\n",
        "Interact with the chatbots and the personas. **Try to think about the following:**\n",
        "\n",
        "*   Do the chatbots follow their persona a lot?\n",
        "*   Was it difficult to follow your persona?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdyaEgpcI7UL",
        "colab_type": "code",
        "outputId": "2794ccfa-8142-4c82-affa-e770c7fa985e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python ~/ParlAI/projects/convai2/interactive.py -mf models:convai2/kvmemnn/model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ warning: overriding opt['model_file'] to /root/ParlAI/data/models/convai2/kvmemnn/model (previously: /checkpoint/jase/20180328/kvmemnn_sweep10/persona-self_rephraseTrn-True_rephraseTst-False_lr-0.1_esz-2000_margin-0.1_tfidf-False_shareEmb-True_hops1_lins0/model )]\n",
            "[ creating KvmemnnAgent ]\n",
            "Dictionary: loading dictionary from /root/ParlAI/data/models/convai2/kvmemnn/model.dict\n",
            "[ num words =  19153 ]\n",
            "Loading existing model params from /root/ParlAI/data/models/convai2/kvmemnn/model\n",
            "[loading candidates: /root/ParlAI/data/models/convai2/kvmemnn/model.candspair*]\n",
            "[caching..]\n",
            "=init done=\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "[creating task(s): parlai.agents.local_human.local_human:LocalHumanAgent]\n",
            "\u001b[1;37;41mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
            "[ optional arguments: ] \n",
            "[  display_examples: False ]\n",
            "[  display_ignore_fields: label_candidates,text_candidates ]\n",
            "[  display_prettify: False ]\n",
            "[ Main ParlAI Arguments: ] \n",
            "[  batchsize: 1 ]\n",
            "[  datapath: /root/ParlAI/data ]\n",
            "[  datatype: train ]\n",
            "[  download_path: /private/home/jase/src/ParlAI/downloads ]\n",
            "[  dynamic_batching: None ]\n",
            "[  hide_labels: False ]\n",
            "[  image_mode: raw ]\n",
            "[  init_opt: None ]\n",
            "[  multitask_weights: [1] ]\n",
            "[  numthreads: 40 ]\n",
            "[  show_advanced_args: False ]\n",
            "[  task: convai2:SelfRevisedTeacher ]\n",
            "[ ParlAI Model Arguments: ] \n",
            "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
            "[  init_model: None ]\n",
            "[  model: projects.personachat.kvmemnn.kvmemnn:KvmemnnAgent ]\n",
            "[  model_file: /root/ParlAI/data/models/convai2/kvmemnn/model ]\n",
            "[ Local Human Arguments: ] \n",
            "[  local_human_candidates_file: None ]\n",
            "[  single_turn: False ]\n",
            "[ ParlAI Image Preprocessing Arguments: ] \n",
            "[  image_cropsize: 224 ]\n",
            "[  image_size: 256 ]\n",
            "[ Dictionary Arguments: ] \n",
            "[  bpe_debug: False ]\n",
            "[  dict_endtoken: __END__ ]\n",
            "[  dict_file: /root/ParlAI/data/models/convai2/kvmemnn/model.dict ]\n",
            "[  dict_initpath: None ]\n",
            "[  dict_language: english ]\n",
            "[  dict_lower: False ]\n",
            "[  dict_max_ngram_size: -1 ]\n",
            "[  dict_maxtokens: -1 ]\n",
            "[  dict_minfreq: 0 ]\n",
            "[  dict_nulltoken: __NULL__ ]\n",
            "[  dict_starttoken: __START__ ]\n",
            "[  dict_textfields: text,labels ]\n",
            "[  dict_tokenizer: split ]\n",
            "[  dict_unktoken: __UNK__ ]\n",
            "[ BPEHelper Arguments: ] \n",
            "[  bpe_add_prefix_space: None ]\n",
            "[  bpe_merge: None ]\n",
            "[  bpe_vocab: None ]\n",
            "[ Kvmemnn Arguments: ] \n",
            "[  cache_size: 1000 ]\n",
            "[  embeddingnorm: 10 ]\n",
            "[  embeddingsize: 2000 ]\n",
            "[  history_length: 100 ]\n",
            "[  history_replies: label ]\n",
            "[  hops: 1 ]\n",
            "[  interactive_mode: False ]\n",
            "[  kvmemnn_debug: False ]\n",
            "[  learningrate: 0.1 ]\n",
            "[  lins: 0 ]\n",
            "[  loadcands: True ]\n",
            "[  loss: cosine ]\n",
            "[  margin: 0.1 ]\n",
            "[  neg_samples: 10 ]\n",
            "[  optimizer: sgd ]\n",
            "[  parrot_neg: 0 ]\n",
            "[  share_embeddings: True ]\n",
            "[  take_next_utt: False ]\n",
            "[  tfidf: False ]\n",
            "[  truncate: -1 ]\n",
            "[  twohop_blend: 0 ]\n",
            "[  twohop_range: 100 ]\n",
            "[creating task(s): convai2:both]\n",
            "[loading fbdialog data:/root/ParlAI/data/ConvAI2/train_both_original.txt]\n",
            "your persona: my favorite is rock fish.\n",
            "your persona: i m going riding on my boat today.\n",
            "your persona: i love tuna.\n",
            "your persona: i live in cape hatteras.\n",
            "your persona: i'm a fisherman.\n",
            "Enter [DONE] if you want a new partner at any time.\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m where are you going?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mprobably to eat thai food . their wives are going , but my fiance is at work .\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m where are you going?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mi am an army brat , so i have been all over . i love traveling , so its great . you ?\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m wow is that your major ?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mchemistry major minor in physics .\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m wow is that your major ?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mchemistry , but have a minor in biology\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m wow is that your major ?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40meducation . i fought in the iraq war and wanted to work on a new career\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m wow is that your major ?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mi think psychology . what was yours ?\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m how many children do you have?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mi have one child a girl well she is a woman now .\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m how many children do you have?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mone . she is two , and the light of my life . do you work out ?\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m how many children do you have?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mi had 5 , but 2 died sadly\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m how many children do you have?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40myou like waiting tables ? i have two sons . they are my world .\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m hi !\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mhello . how are you ? what is your favorite party of history ?\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m who are you ?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mthat would be my mother . she has done so much for me .\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m do you have coronavirus?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mi am good . i just got done practicing the cello , my mom taught me .\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m do you have coronavirus?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mpretty tired . i was up late last night . you ?\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m do you have corona virus?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mhello , i am good . my dog is here with me now .\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m do you have virus?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mi used to have a cat but she died . it made me so sad\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m do you have virus ?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mi translate and edit academic documents , but my mom was a weightlifter .\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m do you have virus ?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mi love dogs ! i have two labradors and a chihuahua .\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m sad\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mshe died too we grew up together upstate , she died from too much excercise aswell\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m happy\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mi enjoy it . he is a horrid cook and i do not want him in my kitchen .\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m sorry\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40msorry to hear that . it must have been so hard\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m fast\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mi have never driven , so no drive thus , my dog died recently\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m you are driving fast \n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mi almost died once driving fast\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m are you driving fast ?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mthat is exciting ! my wife would not like me driving a motorcycle , it would scare the kids .\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m do you know joseph ?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mi have been playing the cello since i was a kid .\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m do you know john ?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mmy mom who is a teacher loves that movie . she was an extra\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m do you know john ?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mmy father died when i was two , which is much worse .\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m john\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mwow ok , do you like to cook ?\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m john\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mdoes she have a reason why ?\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m she is died\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mi am sorry to hear that she passed . she left you with a great skill .\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m are you sick ?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40msick sick , my mothers a teacher which is sick\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m are you married ?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40myes , i am married to my soulmate . she and i have been married 27 years . you ?\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m who is your wife ?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mi guess she is ok , she is listening to books on tape .\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m when did you get married ?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mno , but i have a girlfriend . she lives next door . you ?\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m ask me a question ?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mmy sister died and i am not really sure what you are getting at .\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m ask me a question\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mi can ask her . my kids do not like that she works at their school\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m why\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mi think she found out i am bisexual .\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m Traceback (most recent call last):\n",
            "  File \"/root/ParlAI/projects/convai2/interactive.py\", line 126, in <module>\n",
            "    interactive(parser.parse_args(print_args=False), print_parser=parser)\n",
            "  File \"/root/ParlAI/projects/convai2/interactive.py\", line 102, in interactive\n",
            "    acts[0] = agents[0].act()\n",
            "  File \"/root/ParlAI/parlai/agents/local_human/local_human.py\", line 66, in act\n",
            "    reply_text = input(colorize(\"Enter Your Message:\", 'field') + ' ')\n",
            "KeyboardInterrupt\n",
            "Error in atexit._run_exitfuncs:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/concurrent/futures/thread.py\", line 33, in _python_exit\n",
            "    def _python_exit():\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgBX-qvB4aE9",
        "colab_type": "code",
        "outputId": "140ce279-810c-4667-828d-7dfb71b03866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python ~/ParlAI/projects/convai2/interactive.py -mf models:convai2/kvmemnn/model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ warning: overriding opt['model_file'] to /root/ParlAI/data/models/convai2/kvmemnn/model (previously: /checkpoint/jase/20180328/kvmemnn_sweep10/persona-self_rephraseTrn-True_rephraseTst-False_lr-0.1_esz-2000_margin-0.1_tfidf-False_shareEmb-True_hops1_lins0/model )]\n",
            "[ creating KvmemnnAgent ]\n",
            "Dictionary: loading dictionary from /root/ParlAI/data/models/convai2/kvmemnn/model.dict\n",
            "[ num words =  19153 ]\n",
            "Loading existing model params from /root/ParlAI/data/models/convai2/kvmemnn/model\n",
            "[loading candidates: /root/ParlAI/data/models/convai2/kvmemnn/model.candspair*]\n",
            "[caching..]\n",
            "=init done=\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "[creating task(s): parlai.agents.local_human.local_human:LocalHumanAgent]\n",
            "\u001b[1;37;41mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
            "[ optional arguments: ] \n",
            "[  display_examples: False ]\n",
            "[  display_ignore_fields: label_candidates,text_candidates ]\n",
            "[  display_prettify: False ]\n",
            "[ Main ParlAI Arguments: ] \n",
            "[  batchsize: 1 ]\n",
            "[  datapath: /root/ParlAI/data ]\n",
            "[  datatype: train ]\n",
            "[  download_path: /private/home/jase/src/ParlAI/downloads ]\n",
            "[  dynamic_batching: None ]\n",
            "[  hide_labels: False ]\n",
            "[  image_mode: raw ]\n",
            "[  init_opt: None ]\n",
            "[  multitask_weights: [1] ]\n",
            "[  numthreads: 40 ]\n",
            "[  show_advanced_args: False ]\n",
            "[  task: convai2:SelfRevisedTeacher ]\n",
            "[ ParlAI Model Arguments: ] \n",
            "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
            "[  init_model: None ]\n",
            "[  model: projects.personachat.kvmemnn.kvmemnn:KvmemnnAgent ]\n",
            "[  model_file: /root/ParlAI/data/models/convai2/kvmemnn/model ]\n",
            "[ Local Human Arguments: ] \n",
            "[  local_human_candidates_file: None ]\n",
            "[  single_turn: False ]\n",
            "[ ParlAI Image Preprocessing Arguments: ] \n",
            "[  image_cropsize: 224 ]\n",
            "[  image_size: 256 ]\n",
            "[ Dictionary Arguments: ] \n",
            "[  bpe_debug: False ]\n",
            "[  dict_endtoken: __END__ ]\n",
            "[  dict_file: /root/ParlAI/data/models/convai2/kvmemnn/model.dict ]\n",
            "[  dict_initpath: None ]\n",
            "[  dict_language: english ]\n",
            "[  dict_lower: False ]\n",
            "[  dict_max_ngram_size: -1 ]\n",
            "[  dict_maxtokens: -1 ]\n",
            "[  dict_minfreq: 0 ]\n",
            "[  dict_nulltoken: __NULL__ ]\n",
            "[  dict_starttoken: __START__ ]\n",
            "[  dict_textfields: text,labels ]\n",
            "[  dict_tokenizer: split ]\n",
            "[  dict_unktoken: __UNK__ ]\n",
            "[ BPEHelper Arguments: ] \n",
            "[  bpe_add_prefix_space: None ]\n",
            "[  bpe_merge: None ]\n",
            "[  bpe_vocab: None ]\n",
            "[ Kvmemnn Arguments: ] \n",
            "[  cache_size: 1000 ]\n",
            "[  embeddingnorm: 10 ]\n",
            "[  embeddingsize: 2000 ]\n",
            "[  history_length: 100 ]\n",
            "[  history_replies: label ]\n",
            "[  hops: 1 ]\n",
            "[  interactive_mode: False ]\n",
            "[  kvmemnn_debug: False ]\n",
            "[  learningrate: 0.1 ]\n",
            "[  lins: 0 ]\n",
            "[  loadcands: True ]\n",
            "[  loss: cosine ]\n",
            "[  margin: 0.1 ]\n",
            "[  neg_samples: 10 ]\n",
            "[  optimizer: sgd ]\n",
            "[  parrot_neg: 0 ]\n",
            "[  share_embeddings: True ]\n",
            "[  take_next_utt: False ]\n",
            "[  tfidf: False ]\n",
            "[  truncate: -1 ]\n",
            "[  twohop_blend: 0 ]\n",
            "[  twohop_range: 100 ]\n",
            "[creating task(s): convai2:both]\n",
            "[loading fbdialog data:/root/ParlAI/data/ConvAI2/train_both_original.txt]\n",
            "your persona: my favorite is rock fish.\n",
            "your persona: i m going riding on my boat today.\n",
            "your persona: i love tuna.\n",
            "your persona: i live in cape hatteras.\n",
            "your persona: i'm a fisherman.\n",
            "Enter [DONE] if you want a new partner at any time.\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m where do you live?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mno i do not , i am in russia\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m what do you do?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mi translate and edit academic documents , but my mom was a weightlifter .\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m do you love tuna ?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mmy mom came from russia and she taught me these recipes ! tuna is delicious !\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m what is your favorite\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mthat would be my mother . she has done so much for me .\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m yes\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mhave you ever almost died ? i nearly did from urinary cancer when i was 18 .\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m [DONE]\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mshe is no good . find another\u001b[0;0m\n",
            "CHAT DONE \n",
            "In case you were curious you were talking to this bot:\n",
            "['your persona: she taught me to cook.', 'your persona: she died when i was 18.', 'your persona: my mother is from russia.', 'your persona: i have a dog.', '']\n",
            "\n",
            "... preparing new chat... \n",
            "\n",
            "your persona: i'm a mom with two sons and a husband.\n",
            "your persona: my parents taught me to find inspiration in the world.\n",
            "your persona: i am an artist and create with my mind and my hands.\n",
            "your persona: since my dog died i am incomplete.\n",
            "your persona: i live in a house with a yard.\n",
            "Enter [DONE] if you want a new partner at any time.\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m what do you do?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mdude i start senior year next week . almost done ! you ?\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m what is your major ?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mchemistry major minor in physics .\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m what is your major ?\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mchemistry , but have a minor in biology\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m [DONE]\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mwell i just got done with a worn jam sesh with friends . you like music ?\u001b[0;0m\n",
            "CHAT DONE \n",
            "In case you were curious you were talking to this bot:\n",
            "[\"your persona: i've a friend group , but i don't like any of them.\", 'your persona: but , i feel lost and far away from who i truly am.', 'your persona: i coexist well with my husband.', \"your persona: i'm a middle aged mother of two high school boys.\", '']\n",
            "\n",
            "... preparing new chat... \n",
            "\n",
            "your persona: i like cooking healthy meals.\n",
            "your persona: i like to go out to the bars.\n",
            "your persona: i enjoy dating women.\n",
            "your persona: i am retired.\n",
            "your persona: i like to go to the gym and work out.\n",
            "Enter [DONE] if you want a new partner at any time.\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m Hi\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mhey there . so , have any hobbies ? i enjoy playing video games .\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m hi\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mhey how are you ?\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m good\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mi am pretty well have you been cheated on ?\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m hi\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mhey , how are you ?\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m hi\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mhi how are you ?\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m goof\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mi am just trying to find the beauty in the rest of the world , looking into my yard .\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m good\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mgetting ready for work . i work for a zoo as a keeper , what do you do ?\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m [EXIT]\n",
            "\u001b[1;37;44m[[no id field]]:\u001b[0;0m \u001b[1;37;40mi am on my swing set with my dolls\u001b[0;0m\n",
            "\u001b[1;37;44mEnter Your Message:\u001b[0;0m Traceback (most recent call last):\n",
            "  File \"/root/ParlAI/projects/convai2/interactive.py\", line 126, in <module>\n",
            "    interactive(parser.parse_args(print_args=False), print_parser=parser)\n",
            "  File \"/root/ParlAI/projects/convai2/interactive.py\", line 102, in interactive\n",
            "    acts[0] = agents[0].act()\n",
            "  File \"/root/ParlAI/parlai/agents/local_human/local_human.py\", line 66, in act\n",
            "    reply_text = input(colorize(\"Enter Your Message:\", 'field') + ' ')\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNbf3YEp31Z0",
        "colab_type": "code",
        "outputId": "0833bac5-a706-44d3-c014-889a258b2b42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "%%html\n",
        "<p style='color: blue;'>\n",
        "  <b>Questions:</b>\n",
        "  <ul style='color: blue;'>\n",
        "    <li>What does this model seem to be doing well? What is it doing poorly? </li>\n",
        "    <li>Why might it be performing poorly? What kind of experiment could you design to test your hypothesis?</li>\n",
        "    <li>How do we know if we need to use a more complex model? Would we always want to use a more complex model? Why or why not?</li>\n",
        "  </ul>\n",
        "</p>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style='color: blue;'>\n",
              "  <b>Questions:</b>\n",
              "  <ul style='color: blue;'>\n",
              "    <li>What does this model seem to be doing well? What is it doing poorly? </li>\n",
              "    <li>Why might it be performing poorly? What kind of experiment could you design to test your hypothesis?</li>\n",
              "    <li>How do we know if we need to use a more complex model? Would we always want to use a more complex model? Why or why not?</li>\n",
              "  </ul>\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e21RFV_Y2pew",
        "colab_type": "text"
      },
      "source": [
        "- What does this model seem to be doing well?  \n",
        " **answer :** the model understand the simple interraction\n",
        "\n",
        "- What is it doing poorly?  \n",
        " **answer :** it is not consistent and not fluent\n",
        "\n",
        "- Why might it be performing poorly?  \n",
        " **answer :**\n",
        " - the domain of the bot is limited  \n",
        " - it is just handle basic interraction\n",
        " - it is not able the catch relationship beetween statement in the conversation\n",
        "\n",
        "- What kind of experiment could you design to test your hypothesis?  \n",
        " **answer :**  \n",
        " the experiment :\n",
        " - ask a complex question\n",
        " - ask a question outside of the domain\n",
        " - ask a question and see if the model stay consistent to this answer\n",
        "\n",
        "- How do we know if we need to use a more complex model?  \n",
        " **answer :** by looking at the inability of the model to capture the relationship between the statements\n",
        "\n",
        "- Would we always want to use a more complex model?  \n",
        " **answer :** no\n",
        "\n",
        "- Why or why not?  \n",
        " **answer :** the simple model can perform better than the complex model\n",
        " (it is depending of your baseline)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_SYFM2UaUuq",
        "colab_type": "code",
        "outputId": "173f1e66-6465-4c09-b1b1-45187e604b81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Here is a command to train a Transformer Ranker model if you would like to try it out\n",
        "!python ~/ParlAI/examples/train_model.py -m transformer/ranker -t personachat -dt train -veps 0.25 --model-file persona_chat_retrieval_model -vmt accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ optional arguments: ] \n",
            "[  share_word_embeddings: True ]\n",
            "[ Main ParlAI Arguments: ] \n",
            "[  batchsize: 1 ]\n",
            "[  datapath: /root/ParlAI/data ]\n",
            "[  datatype: train ]\n",
            "[  download_path: /root/ParlAI/downloads ]\n",
            "[  dynamic_batching: None ]\n",
            "[  hide_labels: False ]\n",
            "[  image_mode: raw ]\n",
            "[  init_opt: None ]\n",
            "[  multitask_weights: [1] ]\n",
            "[  numthreads: 1 ]\n",
            "[  show_advanced_args: False ]\n",
            "[  task: personachat ]\n",
            "[ ParlAI Model Arguments: ] \n",
            "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
            "[  init_model: None ]\n",
            "[  model: transformer/ranker ]\n",
            "[  model_file: persona_chat_retrieval_model ]\n",
            "[ Training Loop Arguments: ] \n",
            "[  aggregate_micro: False ]\n",
            "[  display_examples: False ]\n",
            "[  eval_batchsize: None ]\n",
            "[  evaltask: None ]\n",
            "[  load_from_checkpoint: False ]\n",
            "[  max_train_time: -1 ]\n",
            "[  metrics: default ]\n",
            "[  num_epochs: -1 ]\n",
            "[  save_after_valid: False ]\n",
            "[  save_every_n_secs: -1 ]\n",
            "[  short_final_eval: False ]\n",
            "[  validation_cutoff: 1.0 ]\n",
            "[  validation_every_n_epochs: 0.25 ]\n",
            "[  validation_every_n_secs: -1 ]\n",
            "[  validation_max_exs: -1 ]\n",
            "[  validation_metric: accuracy ]\n",
            "[  validation_metric_mode: None ]\n",
            "[  validation_patience: 10 ]\n",
            "[  validation_share_agent: False ]\n",
            "[ Tensorboard Arguments: ] \n",
            "[  tensorboard_log: False ]\n",
            "[ Dictionary Loop Arguments: ] \n",
            "[  dict_include_test: False ]\n",
            "[  dict_include_valid: False ]\n",
            "[  dict_maxexs: -1 ]\n",
            "[  log_every_n_secs: 10 ]\n",
            "[ ParlAI Image Preprocessing Arguments: ] \n",
            "[  image_cropsize: 224 ]\n",
            "[  image_size: 256 ]\n",
            "[ TorchAgent Arguments: ] \n",
            "[  add_p1_after_newln: False ]\n",
            "[  delimiter: \n",
            " ]\n",
            "[  embedding_projection: random ]\n",
            "[  embedding_type: random ]\n",
            "[  force_fp16_tokens: False ]\n",
            "[  fp16: False ]\n",
            "[  fp16_impl: apex ]\n",
            "[  gpu: -1 ]\n",
            "[  history_add_global_end_token: None ]\n",
            "[  history_size: -1 ]\n",
            "[  interactive_mode: False ]\n",
            "[  label_truncate: None ]\n",
            "[  no_cuda: False ]\n",
            "[  person_tokens: False ]\n",
            "[  rank_candidates: False ]\n",
            "[  split_lines: False ]\n",
            "[  text_truncate: None ]\n",
            "[  truncate: 1024 ]\n",
            "[  use_reply: label ]\n",
            "[ Optimizer Arguments: ] \n",
            "[  adafactor_eps: (1e-30, 0.001) ]\n",
            "[  adam_eps: 1e-08 ]\n",
            "[  betas: (0.9, 0.999) ]\n",
            "[  gradient_clip: 0.1 ]\n",
            "[  learningrate: 0.0001 ]\n",
            "[  momentum: 0 ]\n",
            "[  nesterov: True ]\n",
            "[  nus: (0.7,) ]\n",
            "[  optimizer: adamax ]\n",
            "[  weight_decay: None ]\n",
            "[ Learning Rate Scheduler: ] \n",
            "[  invsqrt_lr_decay_gamma: -1 ]\n",
            "[  lr_scheduler: reduceonplateau ]\n",
            "[  lr_scheduler_decay: 0.5 ]\n",
            "[  lr_scheduler_patience: 3 ]\n",
            "[  max_lr_steps: -1 ]\n",
            "[  update_freq: 1 ]\n",
            "[  warmup_rate: 0.0001 ]\n",
            "[  warmup_updates: -1 ]\n",
            "[ TorchRankerAgent: ] \n",
            "[  candidates: inline ]\n",
            "[  cap_num_predictions: 100 ]\n",
            "[  encode_candidate_vecs: True ]\n",
            "[  encode_candidate_vecs_batchsize: 256 ]\n",
            "[  eval_candidates: inline ]\n",
            "[  fixed_candidate_vecs: reuse ]\n",
            "[  fixed_candidates_path: None ]\n",
            "[  ignore_bad_candidates: False ]\n",
            "[  inference: max ]\n",
            "[  init_model: None ]\n",
            "[  interactive_candidates: fixed ]\n",
            "[  rank_top_k: -1 ]\n",
            "[  repeat_blocking_heuristic: True ]\n",
            "[  return_cand_scores: False ]\n",
            "[  topk: 5 ]\n",
            "[  train_predict: False ]\n",
            "[ Transformer Arguments: ] \n",
            "[  activation: relu ]\n",
            "[  attention_dropout: 0.0 ]\n",
            "[  data_parallel: False ]\n",
            "[  dropout: 0.0 ]\n",
            "[  embedding_size: 300 ]\n",
            "[  embeddings_scale: True ]\n",
            "[  ffn_size: 300 ]\n",
            "[  learn_embeddings: True ]\n",
            "[  learn_positional_embeddings: False ]\n",
            "[  memory_attention: sqrt ]\n",
            "[  model_parallel: False ]\n",
            "[  n_decoder_layers: -1 ]\n",
            "[  n_encoder_layers: -1 ]\n",
            "[  n_heads: 2 ]\n",
            "[  n_layers: 2 ]\n",
            "[  n_positions: None ]\n",
            "[  n_segments: 0 ]\n",
            "[  normalize_sent_emb: False ]\n",
            "[  output_scaling: 1.0 ]\n",
            "[  reduction_type: mean ]\n",
            "[  relu_dropout: 0.0 ]\n",
            "[  share_encoders: True ]\n",
            "[  use_memories: False ]\n",
            "[  variant: aiayn ]\n",
            "[  wrap_memory_encoder: False ]\n",
            "[ Dictionary Arguments: ] \n",
            "[  bpe_debug: False ]\n",
            "[  dict_endtoken: __end__ ]\n",
            "[  dict_file: None ]\n",
            "[  dict_initpath: None ]\n",
            "[  dict_language: english ]\n",
            "[  dict_lower: False ]\n",
            "[  dict_max_ngram_size: -1 ]\n",
            "[  dict_maxtokens: -1 ]\n",
            "[  dict_minfreq: 0 ]\n",
            "[  dict_nulltoken: __null__ ]\n",
            "[  dict_starttoken: __start__ ]\n",
            "[  dict_textfields: text,labels ]\n",
            "[  dict_tokenizer: re ]\n",
            "[  dict_unktoken: __unk__ ]\n",
            "[ BPEHelper Arguments: ] \n",
            "[  bpe_add_prefix_space: None ]\n",
            "[  bpe_merge: None ]\n",
            "[  bpe_vocab: None ]\n",
            "[ Current ParlAI commit: 57f946facd540e1ca6113841d3fc0041d7c00460 ]\n",
            "[ building dictionary first... ]\n",
            "[ dictionary already built .]\n",
            "[ no model with opt yet at: persona_chat_retrieval_model(.opt) ]\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from persona_chat_retrieval_model.dict\n",
            "[ num words =  18745 ]\n",
            "Total parameters: 7,197,300 (6,890,100 trainable)\n",
            "[creating task(s): personachat]\n",
            "[loading fbdialog data:/root/ParlAI/data/Persona-Chat/personachat/train_self_original.txt]\n",
            "[ training... ]\n",
            "/root/ParlAI/parlai/core/torch_ranker_agent.py:710: UserWarning: [ Executing train mode with provided inline set of candidates ]\n",
            "  ''.format(mode)\n",
            "/root/ParlAI/parlai/core/torch_ranker_agent.py:470: UserWarning: Some training metrics are omitted for speed. Set the flag `--train-predict` to calculate train metrics.\n",
            "  \"Some training metrics are omitted for speed. Set the flag \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/ParlAI/examples/train_model.py\", line 17, in <module>\n",
            "    TrainLoop(opt).train()\n",
            "  File \"/root/ParlAI/parlai/scripts/train_model.py\", line 642, in train\n",
            "    world.parley()\n",
            "  File \"/root/ParlAI/parlai/core/worlds.py\", line 342, in parley\n",
            "  File \"/root/ParlAI/parlai/core/torch_agent.py\", line 1852, in act\n",
            "    response = self.batch_act([self.observation])[0]\n",
            "  File \"/root/ParlAI/parlai/core/torch_agent.py\", line 1897, in batch_act\n",
            "    output = self.train_step(batch)\n",
            "  File \"/root/ParlAI/parlai/core/torch_ranker_agent.py\", line 446, in train_step\n",
            "    scores = self.score_candidates(batch, cand_vecs)\n",
            "  File \"/root/ParlAI/parlai/agents/transformer/transformer.py\", line 290, in score_candidates\n",
            "    context_h, cands_h = self.model(xs=batch.text_vec, mems=mems, cands=cand_vecs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/root/ParlAI/parlai/agents/transformer/modules.py\", line 297, in forward\n",
            "    cands_h = self.encode_cand(cands)\n",
            "  File \"/root/ParlAI/parlai/agents/transformer/modules.py\", line 250, in encode_cand\n",
            "    encoded = self.cand_encoder(words)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/root/ParlAI/parlai/agents/transformer/modules.py\", line 345, in forward\n",
            "    return self.mlp(self.transformer(*args))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\", line 100, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\", line 87, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\", line 1370, in linear\n",
            "    ret = torch.addmm(bias, input, weight.t())\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd7Sfurfdc_3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "An important aspect of training models is analyzing them. ***Try to answer the following questions.*** \n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1pAkQdt4n5_",
        "colab_type": "code",
        "outputId": "c3c9f03a-4187-41c1-d0c7-626b66557a27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "%%html\n",
        "<p style='color: blue;'>\n",
        "  <b>Questions:</b>\n",
        "  <ul style='color: blue;'>\n",
        "    <li>Are the models using the persona that we have provided? How can you tell? If I asked you to prove it to me, what experiments could you conduct? </li>\n",
        "    <li>Previously, we computed some statistics about how long the persona is in the training data. The model has also only seen words present in the training dataset. But what happens if you push the model outside of what data it's been trained on? What kind of performance do you get? Why does this happen, and what could you do if you wanted to improve the model's ability to generalize? </li>\n",
        "    <li>In ParlAI, we've set the parameters to save the model's best performance based on validation accuracy. What would happen if we saved the model based on the best training accuracy? Why does this happen? (if you like, try this out on your own and see the effect when you interact with the bot)</li>\n",
        "  </ul>\n",
        "\n",
        "<b> If you would like me to discuss how to use BERT in dialog, please say something! Otherwise, we will skip.</b>\n",
        "</p>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style='color: blue;'>\n",
              "  <b>Questions:</b>\n",
              "  <ul style='color: blue;'>\n",
              "    <li>Are the models using the persona that we have provided? How can you tell? If I asked you to prove it to me, what experiments could you conduct? </li>\n",
              "    <li>Previously, we computed some statistics about how long the persona is in the training data. The model has also only seen words present in the training dataset. But what happens if you push the model outside of what data it's been trained on? What kind of performance do you get? Why does this happen, and what could you do if you wanted to improve the model's ability to generalize? </li>\n",
              "    <li>In ParlAI, we've set the parameters to save the model's best performance based on validation accuracy. What would happen if we saved the model based on the best training accuracy? Why does this happen? (if you like, try this out on your own and see the effect when you interact with the bot)</li>\n",
              "  </ul>\n",
              "\n",
              "<b> If you would like me to discuss how to use BERT in dialog, please say something! Otherwise, we will skip.</b>\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WaSo1_J3ohP",
        "colab_type": "text"
      },
      "source": [
        "- Are the models using the persona that we have provided? **No**  \n",
        "How can you tell? **if you ask a specific question related to the provided persona, the response is not related to the persona but is coherent corrected at the whole dataset**  \n",
        "If I asked you to prove it to me, what experiments could you conduct? **check if the response given by the model is related to the persona given**\n",
        "\n",
        "- Previously, we computed some statistics about how long the persona is in the training data. The model has also only seen words present in the training dataset. But what happens if you push the model outside of what data it's been trained on? **the reply given by the model is not related to a statement out of the domain**  \n",
        "What kind of performance do you get? **bad performance**  \n",
        "Why does this happen, and what could you do if you wanted to improve the model's ability to generalize? **because the word is unknown to the model, to improve it we can increase the diversity of the dataset**\n",
        "\n",
        "- In ParlAI, we've set the parameters to save the model's best performance based on validation accuracy. What would happen if we saved the model based on the best training accuracy?  \n",
        "Why does this happen? (if you like, try this out on your own and see the effect when you interact with the bot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTHItY-QdNtw",
        "colab_type": "text"
      },
      "source": [
        "### [for self exploration] Generative Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM6LaBzbe6Za",
        "colab_type": "text"
      },
      "source": [
        "Generative models must produce word for word what they are going to say next in the dialogue. When predicting the next word, it produces a probability distribution over the entire vocabulary space for which word to generate next. To reduce the vocabulary space, we will use **byte-pair encoding** (BPE). \n",
        "\n",
        "*How does BPE work?* The BPE algorithm takes as input the training data and the number of *operations* it can do. It passes over the training set and tries to create sub-word units. For example, the word \"beautiful\" might be split into \"beau\" \"ti\" \"ful\". Each time it splits a word into sub-words, that is one operation. The final vocabulary output consists of these subwords. So \"ful\" can be part of \"beautiful\" and part of \"fruitful\" and so on.\n",
        "\n",
        "\n",
        "**Questions to ask yourself**:\n",
        "\n",
        "\n",
        "1.   Why is it important to keep the vocabulary space small?\n",
        "2.   What does perplexity measure? Why would we use it as a training objective? \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4gNbbAugACx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !python ~/ParlAI/examples/train_model.py -m transformer/generator -t personachat -dt train -veps 0.25 --model-file persona_chat_generative_model -vmt ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHpD0Ccb1zhd",
        "colab_type": "text"
      },
      "source": [
        "## Final Thoughts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHdsZ3Sq12n6",
        "colab_type": "text"
      },
      "source": [
        "**What did we learn about dialogue modeling? Review Questions to ask yourself**\n",
        "\n",
        "*   How do retrieval models work? What about generative? What are their pros and cons?\n",
        "*   What are some important traits of dialogue systems? How might the traits differ for different dialogue tasks?\n",
        "\n",
        "\n",
        "**General Takeaways about Machine Learning and Experimentation:**\n",
        "\n",
        "*   We don't try models just to try them - try to have a reason for conducting an experiment. As we did in the lab, try to analyze what's working well in your models and working poorly. Try to use these reasons to guide why you might want to try other models. Complex is not necessarily better. \n",
        "*   Certain models can be better for certain tasks. As we've seen, generative models are working really well for tasks such as machine translation, but have a bit to go before becoming general purpose dialogue generators. \n",
        "\n",
        "\n",
        "\n",
        "**I'm really interested in dialogue! What can I do to learn more?**\n",
        "\n",
        "\n",
        "*   Play around in ParlAI: ParlAI is a general library with many great dialogue models and code for them. It also provides a standard interface to access datasets and interact with various models. \n",
        "*   Read the PersonaChat Paper: https://arxiv.org/pdf/1801.07243.pdf\n",
        "*   Dialog using knowledge: One challenge of these chit chat systems is they do not concretely know any facts. So if you want to chat about a specific topic, the models cannot produce any relevant information - they say generic utterances or incorrect facts. One way to remedy this is to incorporate **knowledge** into the dialogue agents. This has been investigated in many different ways, but one of the first papers to show this is https://arxiv.org/abs/1811.01241. In this work, data is collected by asking one speaker to reference Wikipedia sentences.\n",
        "* Dialog with BERT: pretty new,  there is an investigation of two ways to use BERT in this paper: https://arxiv.org/abs/1903.03094. \n",
        "\n",
        "\n"
      ]
    }
  ]
}